# Course2 - Natural Language Processing with Probabilistic Models

- Create a simple auto-correct algorithm using minimum edit distance and dynamic programming
- Apply Viterbi algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics
- Write a better auto-complete algorithm using an N-gram language model
- Write your own Word2Vec model that uses a nueral network to compute word embeddings using a continuous bag-of-words model

### Table of Contents

  - [Module1 - Autocorrect](https://github.com/jmcheon/natural_language_processing_specialization/tree/main/Course2/Module1)
  - [Module2 - Part of Speech Tagging and Hidden Markov Models](https://github.com/jmcheon/natural_language_processing_specialization/tree/main/Course2/Module2)
  - [Module3 - Autocomplete and Language Models](https://github.com/jmcheon/natural_language_processing_specialization/tree/main/Course2/Module3)
  - [Module4 - Word Embeddings with Neural Networks](https://github.com/jmcheon/natural_language_processing_specialization/tree/main/Course2/Module4)
<br/>


## Module1 - Autocorrect
Learn about autocorrect, minumum edit distance, and dynamic programming, then build your own spellchecker to correct misspelled words.

- Word Probabilities
- Dynamic Programming
- Minimum Edit Distance
- Autocorrect

## Module2 -  Part of Speech Tagging and Hidden Markov Models

## Module3 - Autocomplete and Language Models

## Module4 - Word Embeddings with Neural Networks